# -*- coding: utf-8 -*-
"""Heart_Disease_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F_ufQAloFyGRG6NL7oTfF2cbT_T8KL2-
"""

# Install necessary libraries (run this only if a library is missing)
!pip install pandas numpy matplotlib seaborn scikit-learn

# Import libraries
import pandas as pd         # For data manipulation
import numpy as np          # For numerical operations
import matplotlib.pyplot as plt  # For plotting graphs
import seaborn as sns       # For statistical data visualization

from sklearn.model_selection import train_test_split  # To split dataset
from sklearn.preprocessing import StandardScaler     # To scale features
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression  # ML model 1
from sklearn.ensemble import RandomForestClassifier  # ML model 2
from sklearn.svm import SVC                           # ML model 3

from google.colab import files

# This will open a file upload dialog
uploaded = files.upload()

import pandas as pd
import io

# Load the uploaded CSV into a DataFrame
df = pd.read_csv(io.BytesIO(uploaded['heart_disease_uci.csv']))

# Display first 5 rows
df.head()

# View dataset info
df.info()

# Check for missing values
df.isnull().sum()

# Summary statistics
df.describe()

import seaborn as sns
import matplotlib.pyplot as plt

# Create a binary target variable from the 'num' column
# 0 = no heart disease, 1 = heart disease (if num > 0)
df['target'] = df['num'].apply(lambda x: 1 if x > 0 else 0)

# Count of patients with and without heart disease
sns.countplot(x='target', data=df)
plt.title('Distribution of Heart Disease (0 = No, 1 = Yes)')
plt.show()

# Summary statistics for all numeric columns
df.describe()

# Check for missing values
df.isnull().sum()

# Correlation heatmap
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm') # Added numeric_only=True
plt.title('Feature Correlation Heatmap')
plt.show()

# Example: Age vs Maximum Heart Rate by target
sns.scatterplot(x='age', y='thalch', hue='target', data=df)
plt.title('Age vs Max Heart Rate by Heart Disease')
plt.show()

# Example: Cholesterol distribution
sns.histplot(df['chol'], kde=True, bins=30)
plt.title('Cholesterol Distribution')
plt.show()

# Features (all columns except 'target')
X = df.drop('target', axis=1)

# Target variable
y = df['target']

from sklearn.model_selection import train_test_split

# 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.impute import SimpleImputer # Import SimpleImputer for handling missing values

scaler = StandardScaler()

# Separate numerical and categorical columns from the training set to identify appropriate preprocessing
numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()
categorical_cols = X_train.select_dtypes(include='object').columns.tolist()

# Impute missing values in numerical columns using the mean of the training set
imputer_numerical = SimpleImputer(strategy='mean')
X_train[numerical_cols] = imputer_numerical.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = imputer_numerical.transform(X_test[numerical_cols])

# Apply one-hot encoding to the categorical features in X_train and X_test
# drop_first=True avoids multicollinearity, and dtype=float ensures numerical output.
X_train_processed = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True, dtype=float)
X_test_processed = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True, dtype=float)

# Ensure that X_test_processed has the same columns and order as X_train_processed
# This handles cases where a category might be present in one set but not the other.
train_cols = X_train_processed.columns
test_cols = X_test_processed.columns

missing_in_test = set(train_cols) - set(test_cols)
for c in missing_in_test:
    X_test_processed[c] = 0

missing_in_train = set(test_cols) - set(train_cols)
for c in missing_in_train:
    X_train_processed[c] = 0

X_test_processed = X_test_processed[train_cols]

# Now, scale the processed (imputed and encoded) numerical features
X_train = scaler.fit_transform(X_train_processed)
X_test = scaler.transform(X_test_processed)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Initialize and train the model
lr_model = LogisticRegression(max_iter=500)
lr_model.fit(X_train, y_train)

# Predict on test data
y_pred_lr = lr_model.predict(X_test)

# Evaluate model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

from sklearn.ensemble import RandomForestClassifier

# Initialize and train the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf_model.predict(X_test)

# Evaluate model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

from sklearn.svm import SVC

# Initialize and train the model
svc_model = SVC(kernel='linear')
svc_model.fit(X_train, y_train)

# Predict on test data
y_pred_svc = svc_model.predict(X_test)

# Evaluate model
print("SVC Accuracy:", accuracy_score(y_test, y_pred_svc))
print(classification_report(y_test, y_pred_svc))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)

# Plot confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

# Example for Logistic Regression
cm_lr = confusion_matrix(y_test, y_pred_lr)
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])
plt.title('Confusion Matrix - Logistic Regression')
plt.show()

# Get feature importance from Random Forest
import pandas as pd

feature_importances = pd.Series(rf_model.feature_importances_, index=X_train_processed.columns)
feature_importances = feature_importances.sort_values(ascending=False)

# Plot feature importance
plt.figure(figsize=(10,6))
sns.barplot(x=feature_importances, y=feature_importances.index)
plt.title('Feature Importance - Random Forest')
plt.show()

from sklearn.model_selection import GridSearchCV

# Example: Random Forest hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
print("Best Accuracy:", grid.best_score_)

import joblib

# Save Random Forest model
joblib.dump(rf_model, 'heart_disease_rf_model.pkl')

# Load model later
# loaded_model = joblib.load('heart_disease_rf_model.pkl')